# è®¾è®¡æ–‡æ¡£ï¼šdaily-arXiv - AIé©±åŠ¨çš„è®ºæ–‡æ¯é¢˜ç­›é€‰å™¨

## ç³»ç»Ÿæ¶æ„

### æ•´ä½“æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      main.py (ä¸»å…¥å£)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. é…ç½®åŠ è½½ (config.py)                                      â”‚
â”‚     â”œâ”€â”€ è¯»å– .env (APIå¯†é’¥ã€Base URL)                        â”‚
â”‚     â””â”€â”€ è¯»å– research_topics.txt (ç ”ç©¶æ¯é¢˜)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. è®ºæ–‡çˆ¬å– (fetcher.py)                                    â”‚
â”‚     â”œâ”€â”€ ä» arXiv RSS è·å–è®ºæ–‡                                â”‚
â”‚     â”œâ”€â”€ è§£æ XML æå–å­—æ®µ                                    â”‚
â”‚     â””â”€â”€ è¿”å› List[Paper]                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. æ¯é¢˜ç­›é€‰ (filter.py)                                     â”‚
â”‚     â”œâ”€â”€ å¯¹æ¯ç¯‡è®ºæ–‡è°ƒç”¨ LLM                                   â”‚
â”‚     â”œâ”€â”€ åˆ¤æ–­æ˜¯å¦åŒ¹é…ç ”ç©¶æ¯é¢˜                                  â”‚
â”‚     â””â”€â”€ è¿”å› List[FilteredPaper]                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. æ‘˜è¦ç”Ÿæˆ (summarizer.py)                                 â”‚
â”‚     â”œâ”€â”€ å¯¹é€šè¿‡ç­›é€‰çš„è®ºæ–‡è°ƒç”¨ LLM                              â”‚
â”‚     â”œâ”€â”€ ç”Ÿæˆä¸­æ–‡æ‘˜è¦                                          â”‚
â”‚     â””â”€â”€ è¿”å› List[SummarizedPaper]                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. è¾“å‡ºæ ¼å¼åŒ– (formatter.py)                                â”‚
â”‚     â”œâ”€â”€ ç”Ÿæˆ Markdown æ ¼å¼                                   â”‚
â”‚     â”œâ”€â”€ ä¿å­˜åˆ° output/YYYY-MM-DD.md                          â”‚
â”‚     â””â”€â”€ æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æ•°æ®ç»“æ„è®¾è®¡

### 1. Paperï¼ˆåŸå§‹è®ºæ–‡ï¼‰

```python
from dataclasses import dataclass
from datetime import datetime

@dataclass
class Paper:
    """arXiv è®ºæ–‡çš„åŸå§‹æ•°æ®"""

    title: str              # è®ºæ–‡æ ‡é¢˜
    abstract: str           # è®ºæ–‡æ‘˜è¦ï¼ˆè‹±æ–‡åŸæ–‡ï¼‰
    authors: list[str]      # ä½œè€…åˆ—è¡¨
    arxiv_id: str           # arXiv IDï¼ˆå¦‚ 2401.12345ï¼‰
    arxiv_url: str          # arXiv é“¾æ¥
    published_date: datetime # å‘å¸ƒæ—¥æœŸ
    categories: list[str]   # ç±»åˆ«æ ‡ç­¾ï¼ˆå¦‚ ["cs.CV", "cs.AI"]ï¼‰

    def __str__(self) -> str:
        return f"{self.title} ({self.arxiv_id})"
```

**è®¾è®¡è¦ç‚¹**ï¼š
- æ‰€æœ‰å­—æ®µéƒ½æ˜¯ä» arXiv RSS è§£æå¾—åˆ°ï¼Œæ— éœ€é¢å¤–è®¡ç®—
- `authors` å­˜å‚¨ä¸ºåˆ—è¡¨ï¼Œæ–¹ä¾¿åç»­æ ¼å¼åŒ–
- `categories` æ”¯æŒå¤šæ ‡ç­¾ï¼ˆä¸€ç¯‡è®ºæ–‡å¯èƒ½å±äºå¤šä¸ªç±»åˆ«ï¼‰

---

### 2. FilteredPaperï¼ˆç­›é€‰åçš„è®ºæ–‡ï¼‰

```python
@dataclass
class FilteredPaper(Paper):
    """é€šè¿‡æ¯é¢˜ç­›é€‰çš„è®ºæ–‡"""

    matched_topics: list[str]  # åŒ¹é…çš„æ¯é¢˜åˆ—è¡¨
    match_reason: str          # åŒ¹é…åŸå› ï¼ˆLLM ç”Ÿæˆï¼‰
    filter_tokens: int         # ç­›é€‰æ¶ˆè€—çš„ token æ•°

    def __str__(self) -> str:
        topics = ", ".join(self.matched_topics)
        return f"{self.title} (åŒ¹é…: {topics})"
```

**è®¾è®¡è¦ç‚¹**ï¼š
- ç»§æ‰¿è‡ª `Paper`ï¼Œåªå¢åŠ ç­›é€‰ç›¸å…³å­—æ®µï¼ˆ"åªå¢ä¸æ”¹"åŸåˆ™ï¼‰
- `matched_topics` æ”¯æŒä¸€ç¯‡è®ºæ–‡åŒ¹é…å¤šä¸ªæ¯é¢˜
- `filter_tokens` ç”¨äºæˆæœ¬ç»Ÿè®¡

---

### 3. SummarizedPaperï¼ˆåŒ…å«æ‘˜è¦çš„è®ºæ–‡ï¼‰

```python
@dataclass
class SummarizedPaper(FilteredPaper):
    """åŒ…å« AI ç”Ÿæˆæ‘˜è¦çš„è®ºæ–‡"""

    summary_zh: str         # ä¸­æ–‡æ‘˜è¦
    summary_tokens: int     # æ‘˜è¦æ¶ˆè€—çš„ token æ•°

    @property
    def total_tokens(self) -> int:
        """æ€» token æ¶ˆè€—ï¼ˆç­›é€‰ + æ‘˜è¦ï¼‰"""
        return self.filter_tokens + self.summary_tokens
```

**è®¾è®¡è¦ç‚¹**ï¼š
- ç»§æ‰¿è‡ª `FilteredPaper`ï¼Œå½¢æˆç»§æ‰¿é“¾ï¼š`Paper â†’ FilteredPaper â†’ SummarizedPaper`
- ä½¿ç”¨ `@property` æä¾›è®¡ç®—å±æ€§ï¼ˆæ€» token æ•°ï¼‰
- æœªæ¥æ‰©å±•æ—¶ï¼Œåªéœ€åœ¨æ­¤åŸºç¡€ä¸Šæ·»åŠ å­—æ®µï¼ˆå¦‚è¢«å¼•é‡ã€ä»£ç é“¾æ¥ç­‰ï¼‰

---

## æ ¸å¿ƒæ¨¡å—è®¾è®¡

### 1. config.py - é…ç½®ç®¡ç†

```python
import os
from pathlib import Path
from dotenv import load_dotenv

class Config:
    """å…¨å±€é…ç½®ç®¡ç†"""

    def __init__(self):
        # åŠ è½½ .env æ–‡ä»¶
        load_dotenv()

        # API é…ç½®
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        self.model_name = os.getenv("MODEL_NAME", "gpt-3.5-turbo")

        # arXiv é…ç½®
        self.arxiv_categories = os.getenv("ARXIV_CATEGORIES", "cs.AI").split(",")
        self.max_papers_per_category = int(os.getenv("MAX_PAPERS_PER_CATEGORY", "50"))

        # è¾“å‡ºé…ç½®
        self.output_dir = Path(os.getenv("OUTPUT_DIR", "output"))
        self.output_language = os.getenv("OUTPUT_LANGUAGE", "zh")

        # ç ”ç©¶æ¯é¢˜
        self.research_topics = self._load_research_topics()

    def _load_research_topics(self) -> list[str]:
        """ä» config/research_topics.txt åŠ è½½ç ”ç©¶æ¯é¢˜"""
        topics_file = Path("config/research_topics.txt")
        if not topics_file.exists():
            raise FileNotFoundError(
                "æœªæ‰¾åˆ° config/research_topics.txtï¼Œè¯·å…ˆåˆ›å»ºè¯¥æ–‡ä»¶å¹¶æ·»åŠ ç ”ç©¶æ¯é¢˜"
            )

        topics = []
        with open(topics_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                # å¿½ç•¥ç©ºè¡Œå’Œæ³¨é‡Š
                if line and not line.startswith("#"):
                    topics.append(line)

        if not topics:
            raise ValueError(
                "ç ”ç©¶æ¯é¢˜åˆ—è¡¨ä¸ºç©ºï¼Œè¯·åœ¨ config/research_topics.txt ä¸­æ·»åŠ è‡³å°‘ä¸€ä¸ªæ¯é¢˜"
            )

        return topics

    def validate(self):
        """éªŒè¯é…ç½®å®Œæ•´æ€§"""
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ° API å¯†é’¥ï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® OPENAI_API_KEY")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        (self.output_dir / "archive").mkdir(exist_ok=True)
```

**è®¾è®¡è¦ç‚¹**ï¼š
- å•ä¸€èŒè´£ï¼šåªè´Ÿè´£é…ç½®åŠ è½½å’ŒéªŒè¯
- é”™è¯¯æç¤ºå‹å¥½ï¼šæŒ‡å‘å…·ä½“çš„é…ç½®æ–‡ä»¶å’Œå­—æ®µ
- è‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆé¿å…è¿è¡Œæ—¶æŠ¥é”™ï¼‰

---

### 2. fetcher.py - è®ºæ–‡çˆ¬å–

```python
import feedparser
import requests
from datetime import datetime
from typing import List
from .models import Paper

def fetch_arxiv_papers(categories: list[str], max_results: int = 50) -> List[Paper]:
    """
    ä» arXiv RSS è·å–æŒ‡å®šç±»åˆ«çš„è®ºæ–‡

    Args:
        categories: arXiv ç±»åˆ«åˆ—è¡¨ï¼ˆå¦‚ ["cs.CV", "cs.AI"]ï¼‰
        max_results: æ¯ä¸ªç±»åˆ«æœ€å¤šçˆ¬å–çš„è®ºæ–‡æ•°

    Returns:
        è®ºæ–‡åˆ—è¡¨

    Raises:
        requests.RequestException: ç½‘ç»œè¿æ¥å¤±è´¥
    """
    papers = []

    for category in categories:
        # æ„å»º arXiv API URL
        # ç¤ºä¾‹ï¼šhttp://export.arxiv.org/api/query?search_query=cat:cs.CV&max_results=50&sortBy=submittedDate&sortOrder=descending
        url = f"http://export.arxiv.org/api/query?search_query=cat:{category.strip()}&max_results={max_results}&sortBy=submittedDate&sortOrder=descending"

        try:
            # è·å–å¹¶è§£æ RSS feed
            feed = feedparser.parse(url)

            # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
            if feed.bozo:
                print(f"âš ï¸  è­¦å‘Šï¼šè§£æç±»åˆ« {category} æ—¶å‡ºé”™ï¼Œè·³è¿‡")
                continue

            # æå–è®ºæ–‡ä¿¡æ¯
            for entry in feed.entries:
                paper = _parse_entry(entry, category)
                if paper:
                    papers.append(paper)

        except requests.RequestException as e:
            raise ConnectionError(
                f"æ— æ³•è¿æ¥åˆ° arXivï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚è¯¦ç»†é”™è¯¯ï¼š{e}"
            )

    return papers


def _parse_entry(entry, category: str) -> Paper | None:
    """
    è§£æå•ä¸ª RSS entry

    Args:
        entry: feedparser.FeedParserDict
        category: arXiv ç±»åˆ«

    Returns:
        Paper å¯¹è±¡æˆ– Noneï¼ˆè§£æå¤±è´¥æ—¶ï¼‰
    """
    try:
        # æå– arXiv IDï¼ˆä» entry.id ä¸­è§£æï¼‰
        # entry.id æ ¼å¼ï¼šhttp://arxiv.org/abs/2401.12345v1
        arxiv_id = entry.id.split("/abs/")[-1].split("v")[0]

        # æå–ä½œè€…åˆ—è¡¨
        authors = [author.name for author in entry.authors]

        # æå–å‘å¸ƒæ—¥æœŸ
        published_date = datetime.strptime(entry.published, "%Y-%m-%dT%H:%M:%SZ")

        # æå–ç±»åˆ«æ ‡ç­¾
        categories = [tag.term for tag in entry.tags]

        return Paper(
            title=entry.title.replace("\n", " ").strip(),
            abstract=entry.summary.replace("\n", " ").strip(),
            authors=authors,
            arxiv_id=arxiv_id,
            arxiv_url=f"https://arxiv.org/abs/{arxiv_id}",
            published_date=published_date,
            categories=categories,
        )

    except Exception as e:
        print(f"âš ï¸  è­¦å‘Šï¼šè§£æè®ºæ–‡æ—¶å‡ºé”™ï¼Œè·³è¿‡ã€‚è¯¦ç»†é”™è¯¯ï¼š{e}")
        return None
```

**è®¾è®¡è¦ç‚¹**ï¼š
- ä½¿ç”¨ `feedparser` è§£æ arXiv RSSï¼ˆæ¯”æ‰‹å†™ XML è§£æç®€å•ï¼‰
- é”™è¯¯å¤„ç†ï¼šç½‘ç»œå¤±è´¥æŠ›å‡ºå¼‚å¸¸ï¼Œå•ç¯‡è§£æå¤±è´¥è·³è¿‡å¹¶è®°å½•
- åˆ†ç¦» `_parse_entry`ï¼šä¾¿äºå•å…ƒæµ‹è¯•

---

### 3. filter.py - æ¯é¢˜ç­›é€‰

```python
from openai import OpenAI
from typing import List
from .models import Paper, FilteredPaper

def filter_papers_by_topics(
    papers: List[Paper],
    topics: List[str],
    llm_client: OpenAI
) -> List[FilteredPaper]:
    """
    ä½¿ç”¨ LLM æ ¹æ®ç ”ç©¶æ¯é¢˜ç­›é€‰è®ºæ–‡

    Args:
        papers: åŸå§‹è®ºæ–‡åˆ—è¡¨
        topics: ç ”ç©¶æ¯é¢˜åˆ—è¡¨
        llm_client: LLM API å®¢æˆ·ç«¯

    Returns:
        é€šè¿‡ç­›é€‰çš„è®ºæ–‡åˆ—è¡¨
    """
    filtered_papers = []

    for i, paper in enumerate(papers, 1):
        print(f"ğŸ” ç­›é€‰è¿›åº¦ï¼š{i}/{len(papers)} - {paper.title[:50]}...")

        # æ„å»º Prompt
        prompt = _build_filter_prompt(paper, topics)

        try:
            # è°ƒç”¨ LLM
            response = llm_client.chat.completions.create(
                model="deepseek-chat",  # æˆ–ä»é…ç½®è¯»å–
                messages=[{"role": "user", "content": prompt}],
                temperature=0,  # é™ä½éšæœºæ€§ï¼Œä¿è¯ä¸€è‡´æ€§
            )

            # è§£æç»“æœ
            result = response.choices[0].message.content.strip()
            tokens = response.usage.total_tokens

            # åˆ¤æ–­æ˜¯å¦åŒ¹é…
            if result.startswith("æ˜¯"):
                # æå–åŒ¹é…çš„æ¯é¢˜å’ŒåŸå› 
                matched_topics, reason = _parse_filter_result(result, topics)

                filtered_papers.append(
                    FilteredPaper(
                        **paper.__dict__,
                        matched_topics=matched_topics,
                        match_reason=reason,
                        filter_tokens=tokens,
                    )
                )
                print(f"  âœ… åŒ¹é…æ¯é¢˜ï¼š{', '.join(matched_topics)}")

        except Exception as e:
            print(f"  âš ï¸  LLM è°ƒç”¨å¤±è´¥ï¼Œè·³è¿‡ã€‚é”™è¯¯ï¼š{e}")
            continue

    return filtered_papers


def _build_filter_prompt(paper: Paper, topics: List[str]) -> str:
    """æ„å»ºæ¯é¢˜ç­›é€‰çš„ Prompt"""
    topics_str = "\n".join([f"{i+1}. {t}" for i, t in enumerate(topics)])

    return f"""è®ºæ–‡æ ‡é¢˜ï¼š{paper.title}
è®ºæ–‡æ‘˜è¦ï¼š{paper.abstract}

ç”¨æˆ·ç ”ç©¶æ¯é¢˜ï¼š
{topics_str}

åˆ¤æ–­ï¼šè¿™ç¯‡è®ºæ–‡æ˜¯å¦ç›´æ¥è§£å†³ä¸Šè¿°ä»»ä¸€æ¯é¢˜?
è¦æ±‚ï¼š
- åªå›ç­”"æ˜¯"æˆ–"å¦"
- å¦‚æœæ˜¯ï¼Œè¯´æ˜åŒ¹é…çš„æ¯é¢˜ç¼–å·å’Œç®€çŸ­åŸå› ï¼ˆ20å­—å†…ï¼‰
- å¦‚æœè®ºæ–‡åªæ˜¯ç•¥å¾®ç›¸å…³ï¼Œä¹Ÿåº”å›ç­”"å¦"

è¾“å‡ºæ ¼å¼ï¼š
æ˜¯/å¦
åŒ¹é…æ¯é¢˜ï¼š[ç¼–å·]
åŸå› ï¼š[ç®€çŸ­è¯´æ˜]
"""


def _parse_filter_result(result: str, topics: List[str]) -> tuple[List[str], str]:
    """
    è§£æ LLM ç­›é€‰ç»“æœ

    Returns:
        (åŒ¹é…çš„æ¯é¢˜åˆ—è¡¨, åŒ¹é…åŸå› )
    """
    lines = result.split("\n")

    # æå–åŒ¹é…çš„æ¯é¢˜ç¼–å·
    matched_indices = []
    reason = ""

    for line in lines:
        if line.startswith("åŒ¹é…æ¯é¢˜"):
            # æå–ç¼–å·ï¼ˆå¦‚ "åŒ¹é…æ¯é¢˜ï¼š1, 3" â†’ [1, 3]ï¼‰
            numbers_str = line.split("ï¼š")[-1].strip()
            matched_indices = [int(n.strip()) for n in numbers_str.split(",")]

        elif line.startswith("åŸå› "):
            reason = line.split("ï¼š")[-1].strip()

    # å°†ç¼–å·è½¬æ¢ä¸ºæ¯é¢˜æ–‡æœ¬
    matched_topics = [topics[i - 1] for i in matched_indices if 0 < i <= len(topics)]

    return matched_topics, reason
```

**è®¾è®¡è¦ç‚¹**ï¼š
- Prompt è®¾è®¡ï¼šæ˜ç¡®è¾“å‡ºæ ¼å¼ï¼Œé™ä½è§£æéš¾åº¦
- æ¸©åº¦è®¾ç½®ä¸º 0ï¼šä¿è¯ç»“æœä¸€è‡´æ€§ï¼ˆä¸éœ€è¦åˆ›é€ æ€§ï¼‰
- é”™è¯¯å¤„ç†ï¼šå•ç¯‡å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹

---

### 4. summarizer.py - æ‘˜è¦ç”Ÿæˆ

```python
from openai import OpenAI
from typing import List
from .models import FilteredPaper, SummarizedPaper

def generate_summaries(
    papers: List[FilteredPaper],
    llm_client: OpenAI
) -> List[SummarizedPaper]:
    """
    ä¸ºé€šè¿‡ç­›é€‰çš„è®ºæ–‡ç”Ÿæˆä¸­æ–‡æ‘˜è¦

    Args:
        papers: é€šè¿‡ç­›é€‰çš„è®ºæ–‡åˆ—è¡¨
        llm_client: LLM API å®¢æˆ·ç«¯

    Returns:
        åŒ…å«æ‘˜è¦çš„è®ºæ–‡åˆ—è¡¨
    """
    summarized_papers = []

    for i, paper in enumerate(papers, 1):
        print(f"ğŸ“ æ‘˜è¦è¿›åº¦ï¼š{i}/{len(papers)} - {paper.title[:50]}...")

        # æ„å»º Prompt
        prompt = _build_summary_prompt(paper)

        try:
            # è°ƒç”¨ LLM
            response = llm_client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,  # ç¨é«˜æ¸©åº¦ï¼Œç”Ÿæˆæ›´æµç•…çš„æ–‡æœ¬
            )

            # æå–æ‘˜è¦
            summary = response.choices[0].message.content.strip()
            tokens = response.usage.total_tokens

            summarized_papers.append(
                SummarizedPaper(
                    **paper.__dict__,
                    summary_zh=summary,
                    summary_tokens=tokens,
                )
            )

        except Exception as e:
            print(f"  âš ï¸  LLM è°ƒç”¨å¤±è´¥ï¼Œè·³è¿‡ã€‚é”™è¯¯ï¼š{e}")
            continue

    return summarized_papers


def _build_summary_prompt(paper: FilteredPaper) -> str:
    """æ„å»ºæ‘˜è¦ç”Ÿæˆçš„ Prompt"""
    topics_str = ", ".join(paper.matched_topics)

    return f"""è®ºæ–‡æ ‡é¢˜ï¼š{paper.title}
è®ºæ–‡æ‘˜è¦ï¼š{paper.abstract}
åŒ¹é…çš„ç ”ç©¶æ¯é¢˜ï¼š{topics_str}

è¯·ç”Ÿæˆä¸€æ®µ 100 å­—çš„ä¸­æ–‡æ‘˜è¦ï¼ŒåŒ…å«ï¼š
1. æ ¸å¿ƒè´¡çŒ®ï¼ˆè¿™ç¯‡è®ºæ–‡è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿï¼‰
2. æŠ€æœ¯æ–¹æ³•ï¼ˆç”¨äº†ä»€ä¹ˆæ–¹æ³•ï¼Ÿï¼‰
3. ä¸ç ”ç©¶æ¯é¢˜çš„å…³è”ï¼ˆä¸ºä»€ä¹ˆåŒ¹é…"{topics_str}"è¿™ä¸ªæ¯é¢˜ï¼Ÿï¼‰

è¦æ±‚ï¼š
- è¯­è¨€ç®€æ´ã€é€šä¿—æ˜“æ‡‚
- é¿å…ä¸“ä¸šæœ¯è¯­å †ç Œ
- çªå‡ºäº®ç‚¹å’Œåˆ›æ–°ç‚¹
"""
```

**è®¾è®¡è¦ç‚¹**ï¼š
- æ¸©åº¦è®¾ç½®ä¸º 0.3ï¼šä¿è¯æµç•…æ€§ï¼ŒåŒæ—¶é¿å…è¿‡åº¦å‘æ•£
- Prompt åŒ…å«åŒ¹é…æ¯é¢˜ï¼šè®©æ‘˜è¦æ›´æœ‰é’ˆå¯¹æ€§

---

### 5. formatter.py - è¾“å‡ºæ ¼å¼åŒ–

```python
from datetime import datetime
from pathlib import Path
from typing import List
from .models import SummarizedPaper

def format_as_markdown(papers: List[SummarizedPaper], stats: dict) -> str:
    """
    å°†è®ºæ–‡åˆ—è¡¨æ ¼å¼åŒ–ä¸º Markdown

    Args:
        papers: åŒ…å«æ‘˜è¦çš„è®ºæ–‡åˆ—è¡¨
        stats: ç»Ÿè®¡ä¿¡æ¯ï¼ˆçˆ¬å–æ•°ã€ç­›é€‰æ•°ã€æˆæœ¬ç­‰ï¼‰

    Returns:
        Markdown æ ¼å¼çš„å­—ç¬¦ä¸²
    """
    # æ ‡é¢˜
    date_str = datetime.now().strftime("%Y-%m-%d")
    md = f"# arXiv æ¯æ—¥ç²¾é€‰ - {date_str}\n\n"

    # ç»Ÿè®¡ä¿¡æ¯
    md += "## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯\n\n"
    md += f"- **çˆ¬å–è®ºæ–‡**ï¼š{stats['total_papers']} ç¯‡\n"
    md += f"- **ç­›é€‰å**ï¼š{stats['filtered_papers']} ç¯‡ï¼ˆç­›é€‰ç‡ï¼š{stats['filter_rate']:.1f}%ï¼‰\n"
    md += f"- **æˆæœ¬**ï¼šÂ¥{stats['total_cost']:.2f}\n"
    md += f"- **è¿è¡Œæ—¶é—´**ï¼š{stats['runtime']}\n\n"
    md += "---\n\n"

    # è®ºæ–‡åˆ—è¡¨
    md += "## ğŸ“„ è®ºæ–‡åˆ—è¡¨\n\n"

    for i, paper in enumerate(papers, 1):
        md += f"### {i}. [{paper.title}]({paper.arxiv_url})\n\n"
        md += f"**ä½œè€…**ï¼š{', '.join(paper.authors[:3])}{'...' if len(paper.authors) > 3 else ''}\n\n"
        md += f"**åŒ¹é…æ¯é¢˜**ï¼š{', '.join(paper.matched_topics)}\n\n"
        md += f"**æ‘˜è¦**ï¼š\n{paper.summary_zh}\n\n"
        md += f"**arXiv**ï¼š[{paper.arxiv_id}]({paper.arxiv_url})  \n"
        md += f"**å‘å¸ƒæ—¥æœŸ**ï¼š{paper.published_date.strftime('%Y-%m-%d')}\n\n"
        md += "---\n\n"

    return md


def save_to_file(content: str, output_dir: Path) -> str:
    """
    ä¿å­˜åˆ°æ–‡ä»¶

    Args:
        content: Markdown å†…å®¹
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    # ç”Ÿæˆæ–‡ä»¶å
    date_str = datetime.now().strftime("%Y-%m-%d")
    file_path = output_dir / f"{date_str}.md"

    # ä¿å­˜
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(content)

    return str(file_path)
```

**è®¾è®¡è¦ç‚¹**ï¼š
- Markdown æ ¼å¼é€‚åˆå¯¼å…¥ Obsidian
- ä½œè€…åˆ—è¡¨è¶…è¿‡ 3 äººæ—¶çœç•¥ï¼ˆé¿å…è¿‡é•¿ï¼‰
- ä½¿ç”¨ `---` åˆ†éš”è®ºæ–‡ï¼ˆæé«˜å¯è¯»æ€§ï¼‰

---

## æ€§èƒ½ä¼˜åŒ–ï¼ˆäºŒæœŸï¼‰

**å½“å‰ä¸åšï¼Œä½†ä¿ç•™æ¥å£**ï¼š

### 1. å¹¶å‘å¤„ç†

```python
import asyncio
from openai import AsyncOpenAI

async def filter_papers_async(papers: List[Paper], topics: List[str]) -> List[FilteredPaper]:
    """å¼‚æ­¥ç­›é€‰è®ºæ–‡ï¼ˆå¹¶å‘è°ƒç”¨ LLMï¼‰"""
    async with AsyncOpenAI() as client:
        tasks = [_filter_single_paper(p, topics, client) for p in papers]
        results = await asyncio.gather(*tasks)
    return [r for r in results if r is not None]
```

### 2. ç¼“å­˜æœºåˆ¶

```python
import sqlite3
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_cached_summary(arxiv_id: str) -> str | None:
    """ä»ç¼“å­˜ä¸­è·å–æ‘˜è¦ï¼ˆé¿å…é‡å¤è°ƒç”¨ LLMï¼‰"""
    conn = sqlite3.connect("cache.db")
    cursor = conn.execute("SELECT summary FROM papers WHERE arxiv_id = ?", (arxiv_id,))
    result = cursor.fetchone()
    conn.close()
    return result[0] if result else None
```

---

## é”™è¯¯å¤„ç†ç­–ç•¥

### 1. API è°ƒç”¨å¤±è´¥

```python
import time
from openai import OpenAIError

def call_llm_with_retry(prompt: str, max_retries: int = 3) -> str:
    """å¸¦é‡è¯•çš„ LLM è°ƒç”¨"""
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(...)
            return response.choices[0].message.content

        except OpenAIError as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                print(f"âš ï¸  LLM è°ƒç”¨å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                raise
```

### 2. ç½‘ç»œè¶…æ—¶

```python
import requests

def fetch_with_timeout(url: str, timeout: int = 10) -> str:
    """å¸¦è¶…æ—¶çš„ HTTP è¯·æ±‚"""
    try:
        response = requests.get(url, timeout=timeout)
        response.raise_for_status()
        return response.text

    except requests.Timeout:
        raise TimeoutError(f"è¯·æ±‚è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")

    except requests.RequestException as e:
        raise ConnectionError(f"ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼š{e}")
```

---

## æˆæœ¬ä¼°ç®—

### Token è®¡ç®—

```python
def estimate_cost(papers: List[SummarizedPaper]) -> dict:
    """ä¼°ç®—æˆæœ¬"""
    total_tokens = sum(p.total_tokens for p in papers)

    # DeepSeek ä»·æ ¼ï¼šÂ¥0.001/1K tokens
    total_cost = (total_tokens / 1000) * 0.001

    return {
        "total_tokens": total_tokens,
        "total_cost": total_cost,
        "avg_tokens_per_paper": total_tokens / len(papers) if papers else 0,
    }
```

---

## æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•

```python
# tests/test_filter.py
import pytest
from src.filter import _parse_filter_result

def test_parse_filter_result():
    """æµ‹è¯•ç­›é€‰ç»“æœè§£æ"""
    topics = ["AI Agent çš„å·¥å…·è°ƒç”¨ä¼˜åŒ–", "é•¿è§†é¢‘ç†è§£çš„æ•ˆç‡é—®é¢˜"]
    result = """æ˜¯
åŒ¹é…æ¯é¢˜ï¼š1
åŸå› ï¼šæå‡ºäº†æ–°çš„å·¥å…·è°ƒç”¨ä¼˜åŒ–æ–¹æ³•"""

    matched_topics, reason = _parse_filter_result(result, topics)

    assert matched_topics == ["AI Agent çš„å·¥å…·è°ƒç”¨ä¼˜åŒ–"]
    assert reason == "æå‡ºäº†æ–°çš„å·¥å…·è°ƒç”¨ä¼˜åŒ–æ–¹æ³•"
```

### é›†æˆæµ‹è¯•

```python
# tests/test_integration.py
def test_full_pipeline(tmp_path):
    """æµ‹è¯•å®Œæ•´æµç¨‹"""
    # 1. åˆ›å»ºæµ‹è¯•é…ç½®
    config = Config()
    config.output_dir = tmp_path

    # 2. çˆ¬å–è®ºæ–‡ï¼ˆä½¿ç”¨ mockï¼‰
    papers = fetch_arxiv_papers(["cs.AI"], max_results=10)
    assert len(papers) > 0

    # 3. ç­›é€‰
    filtered = filter_papers_by_topics(papers, ["AI Agent"], llm_client)
    assert len(filtered) > 0

    # 4. æ‘˜è¦
    summarized = generate_summaries(filtered, llm_client)
    assert len(summarized) == len(filtered)

    # 5. è¾“å‡º
    md = format_as_markdown(summarized, stats)
    file_path = save_to_file(md, tmp_path)

    assert Path(file_path).exists()
```

---

## ç›¸å…³æ–‡æ¡£

- **[requirements.md](requirements.md)**ï¼šéœ€æ±‚æ–‡æ¡£ï¼ˆç”¨æˆ·æ•…äº‹ã€éªŒæ”¶æ ‡å‡†ï¼‰
- **[../CLAUDE.md](../CLAUDE.md)**ï¼šé¡¹ç›®æ€»è§ˆï¼ˆç»™ AI å’Œå¼€å‘è€…çœ‹ï¼‰
- **[../README.md](../README.md)**ï¼šç”¨æˆ·ä½¿ç”¨æŒ‡å—
