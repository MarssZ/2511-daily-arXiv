{"id": "2511.03773", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03773", "abs": "https://arxiv.org/abs/2511.03773", "authors": ["Zhaorun Chen", "Zhuokai Zhao", "Kai Zhang", "Bo Liu", "Qi Qi", "Yifan Wu", "Tarun Kalluri", "Sara Cao", "Yuanhao Xiong", "Haibo Tong", "Huaxiu Yao", "Hengduo Li", "Jiacheng Zhu", "Xian Li", "Dawn Song", "Bo Li", "Jason Weston", "Dat Huynh"], "title": "Scaling Agent Learning via Experience Synthesis", "comment": null, "summary": "While reinforcement learning (RL) can empower large language model (LLM)\nagents by enabling self-improvement through interaction, its practical adoption\nremains challenging due to costly rollouts, limited task diversity, unreliable\nreward signals, and infrastructure complexity, all of which obstruct the\ncollection of scalable experience data. To address these challenges, we\nintroduce DreamGym, the first unified framework designed to synthesize diverse\nexperiences with scalability in mind to enable effective online RL training for\nautonomous agents. Rather than relying on expensive real-environment rollouts,\nDreamGym distills environment dynamics into a reasoning-based experience model\nthat derives consistent state transitions and feedback signals through\nstep-by-step reasoning, enabling scalable agent rollout collection for RL. To\nimprove the stability and quality of transitions, DreamGym leverages an\nexperience replay buffer initialized with offline real-world data and\ncontinuously enriched with fresh interactions to actively support agent\ntraining. To improve knowledge acquisition, DreamGym adaptively generates new\ntasks that challenge the current agent policy, enabling more effective online\ncurriculum learning. Experiments across diverse environments and agent\nbackbones demonstrate that DreamGym substantially improves RL training, both in\nfully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready\ntasks like WebArena, DreamGym outperforms all baselines by over 30%. And in\nRL-ready but costly settings, it matches GRPO and PPO performance using only\nsynthetic interactions. When transferring a policy trained purely on synthetic\nexperiences to real-environment RL, DreamGym yields significant additional\nperformance gains while requiring far fewer real-world interactions, providing\na scalable warm-start strategy for general-purpose RL.", "AI": {"tldr": "DreamGym\u662f\u4e00\u4e2a\u9762\u5411\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u7406\u578b\u7ecf\u9a8c\u6a21\u578b\u751f\u6210\u53ef\u6269\u5c55\u7684\u591a\u6837\u5316\u5408\u6210\u7ecf\u9a8c\uff0c\u51cf\u5c11\u5bf9\u6602\u8d35\u771f\u5b9e\u73af\u5883\u4ea4\u4e92\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u7684\u8bad\u7ec3\u6548\u7387\u4e0e\u6027\u80fd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u4e2d\u5177\u6709\u81ea\u6539\u8fdb\u6f5c\u529b\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u53d7\u9650\u4e8e\u9ad8\u6602\u7684 rollout \u6210\u672c\u3001\u4efb\u52a1\u591a\u6837\u6027\u4e0d\u8db3\u3001\u5956\u52b1\u4fe1\u53f7\u4e0d\u53ef\u9760\u548c\u57fa\u7840\u8bbe\u65bd\u590d\u6742\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u83b7\u53d6\u53ef\u6269\u5c55\u7684\u7ecf\u9a8c\u6570\u636e\u3002", "method": "DreamGym\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u7684\u7ecf\u9a8c\u6a21\u578b\uff0c\u901a\u8fc7\u9010\u6b65\u63a8\u7406\u6a21\u62df\u72b6\u6001\u8f6c\u79fb\u548c\u53cd\u9988\u4fe1\u53f7\uff0c\u66ff\u4ee3\u771f\u5b9e\u73af\u5883rollout\uff1b\u7ed3\u5408\u79bb\u7ebf\u771f\u5b9e\u6570\u636e\u521d\u59cb\u5316\u7684\u7ecf\u9a8c\u56de\u653e\u7f13\u51b2\u533a\uff0c\u5e76\u6301\u7eed\u52a0\u5165\u65b0\u4ea4\u4e92\u6570\u636e\uff1b\u540c\u65f6\u81ea\u9002\u5e94\u751f\u6210\u6311\u6218\u6027\u65b0\u4efb\u52a1\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5728\u7ebf\u8bfe\u7a0b\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDreamGym\u5728\u591a\u79cd\u73af\u5883\u548c\u6a21\u578b\u7ed3\u6784\u4e0b\u663e\u8457\u63d0\u5347RL\u8bad\u7ec3\u6548\u679c\uff1a\u5728WebArena\u7b49\u975eRL\u5c31\u7eea\u4efb\u52a1\u4e0a\u6027\u80fd\u8d85\u8fc7\u57fa\u7ebf30%\u4ee5\u4e0a\uff1b\u5728RL\u5c31\u7eea\u4f46\u6210\u672c\u9ad8\u7684\u573a\u666f\u4e2d\uff0c\u4ec5\u7528\u5408\u6210\u4ea4\u4e92\u5373\u53ef\u8fbe\u5230GRPO\u548cPPO\u7684\u6027\u80fd\uff1b\u5728\u7eaf\u5408\u6210\u8bad\u7ec3\u7b56\u7565\u8fc1\u79fb\u5230\u771f\u5b9e\u73af\u5883\u65f6\uff0c\u4ecd\u80fd\u53d6\u5f97\u663e\u8457\u6027\u80fd\u589e\u76ca\u4e14\u6240\u9700\u771f\u5b9e\u4ea4\u4e92\u66f4\u5c11\u3002", "conclusion": "DreamGym\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u7ecf\u9a8c\u5b9e\u73b0\u9ad8\u6027\u80fd\u7b56\u7565\u8bad\u7ec3\uff0c\u5e76\u652f\u6301\u4f4e\u4ee3\u4ef7\u7684\u73b0\u5b9e\u8fc1\u79fb\uff0c\u662f\u4e00\u79cd\u53ef\u884c\u7684\u5f3a\u5316\u5b66\u4e60 warm-start \u7b56\u7565\u3002"}}
{"id": "2511.03825", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03825", "abs": "https://arxiv.org/abs/2511.03825", "authors": ["Ahmed Mostafa", "Raisul Arefin Nahid", "Samuel Mulder"], "title": "How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis", "comment": "Publication Notice. This paper was published in the BAR 2025 Workshop\n  (with NDSS 2025) and is for research and educational use. Copyright\n  \\c{opyright} 2025 Internet Society. All rights reserved. Personal/classroom\n  reproduction is permitted with this notice and full paper citation. All other\n  uses, including commercial, require prior written permission from the\n  Internet Society", "summary": "Tokenization is fundamental in assembly code analysis, impacting intrinsic\ncharacteristics like vocabulary size, semantic coverage, and extrinsic\nperformance in downstream tasks. Despite its significance, tokenization in the\ncontext of assembly code remains an underexplored area. This study aims to\naddress this gap by evaluating the intrinsic properties of Natural Language\nProcessing (NLP) tokenization models and parameter choices, such as vocabulary\nsize. We explore preprocessing customization options and pre-tokenization rules\ntailored to the unique characteristics of assembly code. Additionally, we\nassess their impact on downstream tasks like function signature prediction -- a\ncritical problem in binary code analysis.\n  To this end, we conduct a thorough study on various tokenization models,\nsystematically analyzing their efficiency in encoding assembly instructions and\ncapturing semantic nuances. Through intrinsic evaluations, we compare\ntokenizers based on tokenization efficiency, vocabulary compression, and\nrepresentational fidelity for assembly code. Using state-of-the-art pre-trained\nmodels such as the decoder-only Large Language Model (LLM) Llama 3.2, the\nencoder-only transformer BERT, and the encoder-decoder model BART, we evaluate\nthe effectiveness of these tokenizers across multiple performance metrics.\nPreliminary findings indicate that tokenizer choice significantly influences\ndownstream performance, with intrinsic metrics providing partial but incomplete\npredictability of extrinsic evaluation outcomes. These results reveal complex\ntrade-offs between intrinsic tokenizer properties and their utility in\npractical assembly code tasks. Ultimately, this study provides valuable\ninsights into optimizing tokenization models for low-level code analysis,\ncontributing to the robustness and scalability of Natural Language Model\n(NLM)-based binary analysis workflows.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u6c47\u7f16\u4ee3\u7801\u5206\u6790\u4e2d\u7684\u5206\u8bcd\u6280\u672f\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540cNLP\u5206\u8bcd\u6a21\u578b\u53ca\u53c2\u6570\u9009\u62e9\u5bf9\u6c47\u7f16\u4ee3\u7801\u5185\u5728\u7279\u6027\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5206\u8bcd\u5668\u7684\u9009\u62e9\u663e\u8457\u5f71\u54cd\u4efb\u52a1\u8868\u73b0\uff0c\u4e14\u5185\u5728\u6307\u6807\u4ec5\u80fd\u90e8\u5206\u9884\u6d4b\u5b9e\u9645\u6548\u679c\uff0c\u4e3a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u7684\u4e8c\u8fdb\u5236\u5206\u6790\u63d0\u4f9b\u4e86\u4f18\u5316\u601d\u8def\u3002", "motivation": "\u6c47\u7f16\u4ee3\u7801\u7684\u5206\u8bcd\u6280\u672f\u5728\u8bcd\u6c47\u5927\u5c0f\u3001\u8bed\u4e49\u8986\u76d6\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f46\u76ee\u524d\u8be5\u9886\u57df\u7814\u7a76\u4e0d\u8db3\uff0c\u4e9f\u9700\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u5206\u8bcd\u6a21\u578b\u53ca\u5176\u53c2\u6570\u5bf9\u6c47\u7f16\u4ee3\u7801\u5206\u6790\u7684\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5185\u5728\u8bc4\u4f30\u6bd4\u8f83\u4e0d\u540c\u5206\u8bcd\u5668\u5728\u5206\u8bcd\u6548\u7387\u3001\u8bcd\u6c47\u538b\u7f29\u548c\u8868\u793a\u4fdd\u771f\u5ea6\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u5229\u7528Llama 3.2\u3001BERT\u548cBART\u7b49\u5148\u8fdb\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u51fd\u6570\u7b7e\u540d\u9884\u6d4b\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8fdb\u884c\u5916\u5728\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u5206\u8bcd\u5668\u7684\u9009\u62e9\u663e\u8457\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u5185\u5728\u6307\u6807\uff08\u5982\u8bcd\u6c47\u538b\u7f29\u7387\uff09\u4e0e\u5916\u5728\u4efb\u52a1\u8868\u73b0\u4e4b\u95f4\u5b58\u5728\u590d\u6742\u6743\u8861\uff0c\u67d0\u4e9b\u5206\u8bcd\u7b56\u7565\u5728\u4fdd\u6301\u9ad8\u6548\u7f16\u7801\u7684\u540c\u65f6\u63d0\u5347\u4e86\u8bed\u4e49\u6355\u6349\u80fd\u529b\u3002", "conclusion": "\u9488\u5bf9\u6c47\u7f16\u4ee3\u7801\u7279\u6027\u5b9a\u5236\u7684\u5206\u8bcd\u65b9\u6848\u66f4\u6709\u5229\u4e8e\u4f4e\u7ea7\u4ee3\u7801\u5206\u6790\uff0c\u5408\u7406\u9009\u62e9\u5206\u8bcd\u6a21\u578b\u548c\u53c2\u6570\u53ef\u63d0\u5347NLM\u5728\u4e8c\u8fdb\u5236\u5206\u6790\u4e2d\u7684\u9c81\u68d2\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
