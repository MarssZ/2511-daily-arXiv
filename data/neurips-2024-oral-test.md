<div id=toc></div>

# Table of Contents

- [NeurIPS 2024 Oral](#NeurIPS 2024 Oral) [Total: 6]


<div id='NeurIPS 2024 Oral'></div>

# NeurIPS 2024 Oral [[Back]](#toc)

### [1] [Policy Learning from Tutorial Books via Understanding, Rehearsing and Introspecting](https://neurips.cc/virtual/2024/oral/97989)
**é«˜ä»·å€¼** | *Xiong-Hui Chen, Ziyan Wang, Yali Du, Shengyi Jiang, Meng Fang, Yang Yu, Jun Wang*

**ğŸ’¡ å¤§ç™½è¯**: å°±åƒå°æœ‹å‹çœ‹ä¹¦å­¦ä¸‹æ£‹ï¼Œå…ˆè¯»æ‡‚è§„åˆ™ï¼Œç„¶ååœ¨è„‘å­é‡Œæ‰“å‡ ç›˜ï¼Œæœ€åæ€»ç»“å‡ºè‡ªå·±çš„æ‹›æ•°ï¼Œè¿™ä¸ªæ–¹æ³•è®©AIä¹Ÿèƒ½â€˜è¯»ä¹¦è‡ªå­¦â€™ç©æ¸¸æˆã€‚

**ğŸ¯ æ ¸å¿ƒä»·å€¼**: é’ˆå¯¹å¼ºåŒ–å­¦ä¹ éš¾ä»¥åˆ©ç”¨æ–‡æœ¬çŸ¥è¯†çš„æ ¹æœ¬ç“¶é¢ˆï¼Œæœ¬æ–‡æå‡ºå—äººç±»å­¦ä¹ å¯å‘çš„â€˜ç†è§£-æ’ç»ƒ-åæ€â€™ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡å°†æ•™ç¨‹å†…å®¹è½¬åŒ–ä¸ºè™šæ‹Ÿå†³ç­–ç»éªŒå¹¶è’¸é¦æˆç­–ç•¥ç½‘ç»œï¼Œåœ¨æ— éœ€çœŸå®äº¤äº’çš„æƒ…å†µä¸‹å®ç°äº†é«˜æ•ˆç­–ç•¥å­¦ä¹ ã€‚

**ğŸ“Š ä¸»ç±»åˆ«**: NeurIPS 2024 Oral

<details>
  <summary><b>ğŸ“– è¯¦ç»†åˆ†æ</b></summary>

#### ğŸ” é—®é¢˜ä¸æ´å¯Ÿ
- **æ ¹æœ¬é—®é¢˜**: å¼ºåŒ–å­¦ä¹ ä¾èµ–å¤§é‡ç¯å¢ƒäº¤äº’è·å–æŠ€èƒ½ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨å·²å­˜åœ¨çš„æ–‡æœ¬åŒ–çŸ¥è¯†ï¼ˆå¦‚æ•™ç¨‹ã€ä¹¦ç±ï¼‰ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚
- **åˆ‡å…¥è§†è§’**: äººç±»é€šè¿‡é˜…è¯»ç†è§£â†’å¿ƒç†æ’ç»ƒâ†’åæ€å†…åŒ–æ¥å­¦ä¹ æŠ€èƒ½ï¼Œè¿™ä¸€è®¤çŸ¥è¿‡ç¨‹å¯è¢«å»ºæ¨¡ä¸ºæœºå™¨å†³ç­–å­¦ä¹ çš„ä¸‰é˜¶æ®µæ¡†æ¶ã€‚

#### âš™ï¸ æ–¹æ³•ä¸å‘ç°
- **å…³é”®æ–¹æ³•**: æå‡ºURIä¸‰é˜¶æ®µæ¡†æ¶ï¼š1ï¼‰ç†è§£ï¼ˆLLMè§£ææ–‡æœ¬è·å–çŸ¥è¯†ï¼‰ï¼›2ï¼‰æ’ç»ƒï¼ˆç”Ÿæˆè™šæ‹Ÿå†³ç­–è½¨è¿¹ï¼‰ï¼›3ï¼‰åæ€ï¼ˆåœ¨æƒ³è±¡æ•°æ®é›†ä¸Šè’¸é¦ç­–ç•¥ç½‘ç»œï¼‰ã€‚
- **æ–¹æ³•å…¬å¼**: ç­–ç•¥å­¦ä¹  = ç†è§£(æ–‡æœ¬ â†’ çŸ¥è¯†) â†’ æ’ç»ƒ(çŸ¥è¯† â†’ è™šæ‹Ÿè½¨è¿¹) â†’ åæ€(è½¨è¿¹ â†’ ç­–ç•¥ç½‘ç»œ)
- **æ ¸å¿ƒå‘ç°**: åœ¨æ— çœŸå®äº¤äº’æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒURIèƒ½ä»æ•™ç¨‹ä¹¦ä¸­å­¦ä¹ æœ‰æ•ˆç­–ç•¥ï¼Œåœ¨äº•å­—æ£‹å’Œè¶³çƒæ¸¸æˆä¸­æ˜¾è‘—è¶…è¶ŠåŸºäºGPTçš„åŸºçº¿æ¨¡å‹ã€‚

#### ğŸ’ ä»·å€¼è¯„ä¼°
- **æœºåˆ¶æ´å¯Ÿ**: æ­ç¤ºäº†â€˜ç¦»çº¿çŸ¥è¯†åˆ°åœ¨çº¿å†³ç­–â€™çš„è½¬åŒ–æœºåˆ¶ï¼šæ–‡æœ¬ä¸­çš„éšæ€§è§„åˆ™å¯é€šè¿‡ç»“æ„åŒ–æ¨¡æ‹Ÿè½¬åŒ–ä¸ºå¯æ‰§è¡Œç­–ç•¥ã€‚æ”¹å˜äº†RLå¿…é¡»ä¾èµ–åœ¨çº¿è¯•é”™çš„è®¤çŸ¥æ¨¡å‹ï¼Œç¡®ç«‹äº†â€˜æƒ³è±¡è®­ç»ƒâ€™ä½œä¸ºå¯è¡Œå­¦ä¹ è·¯å¾„ã€‚
- **è¡ŒåŠ¨å¯å‘**: è·¨è¶Šå¼æ”¹è¿›ï¼ˆ5-10xæå‡ï¼‰ã€‚æä¾›å¯æ“ä½œè§„åˆ™ï¼š1ï¼‰ç”¨LLMåšçŸ¥è¯†æå–å™¨è€Œéç›´æ¥æ§åˆ¶å™¨ï¼›2ï¼‰æ„å»ºè™šæ‹Ÿç»éªŒæ± è¿›è¡Œç­–ç•¥è’¸é¦ï¼›3ï¼‰åˆ†ç¦»çŸ¥è¯†è·å–ä¸ç­–ç•¥ä¼˜åŒ–é˜¶æ®µã€‚å¯åœ¨æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆAIã€æ•™è‚²ç³»ç»Ÿä¸­ç«‹å³éªŒè¯ã€‚
- **å¯è¿ç§»æ€§**: 1ï¼‰åŒ»å­¦è¯Šç–—è·¯å¾„å­¦ä¹ ï¼ˆä»æŒ‡å—æ–‡æœ¬ç”Ÿæˆè™šæ‹Ÿç—…ä¾‹å†³ç­–æµï¼‰ï¼›2ï¼‰å·¥ä¸šæ•…éšœå¤„ç½®ï¼ˆä»æ“ä½œæ‰‹å†Œæ„å»ºåº”æ€¥å“åº”ç­–ç•¥ï¼‰ï¼›3ï¼‰è‡ªåŠ¨é©¾é©¶è¡Œä¸ºè§„åˆ’ï¼ˆä»äº¤è§„ä¸é©¾é©¶æ‰‹å†Œæ¨å¯¼é©¾é©¶ç­–ç•¥ï¼‰ã€‚

#### ğŸ“„ åŸæ–‡æ‘˜è¦
When humans need to learn a new skill, we can acquire knowledge through written books, including textbooks, tutorials, etc. However, current research for decision-making, like reinforcement learning (RL), has primarily required numerous real interactions with the target environment to learn a skill, while failing to utilize the existing knowledge already summarized in the text. The success of Large Language Models (LLMs) sheds light on utilizing such knowledge behind the books. In this paper, we discuss a new policy learning problem called Policy Learning from tutorial Books (PLfB) upon the shoulders of LLMsâ€™ systems, which aims to leverage rich resources such as tutorial books to derive a policy network. Inspired by how humans learn from books, we solve the problem via a three-stage framework: Understanding, Rehearsing, and Introspecting (URI). In particular, it first rehearses decision-making trajectories based on the derived knowledge after understanding the books, then introspects in the imaginary dataset to distill a policy network.  We build two benchmarks for PLfB~based on Tic-Tac-Toe and Football games. In experiment, URI's policy achieves at least 44% net win rate against GPT-based agents without any real data; In Football game, which is a complex scenario, URI's policy beat the built-in AIs with a â€¦

</details>


### [2] [MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map](https://neurips.cc/virtual/2024/oral/97971)
**é«˜ä»·å€¼** | *YUHONG CHOU, Man Yao, Kexin Wang, Yuqi Pan, Rui-Jie Zhu, Jibin Wu, Yiran Zhong, Yu Qiao, Bo Xu, Guoqi Li*

**ğŸ’¡ å¤§ç™½è¯**: å°±åƒç”¨ä¾¿å®œææ–™åšä¸€å¼ èƒ½å˜å½¢çš„æ¤…å­ï¼Œä»¥å‰è¦ä¹ˆå¤ªé‡ï¼Œè¦ä¹ˆä¸èƒ½åŠ¨ï¼Œç°åœ¨æ‰¾åˆ°äº†ä¸€ç§åˆè½»åˆèƒ½æ ¹æ®äººå½¢å˜çš„æ–¹æ³•ã€‚

**ğŸ¯ æ ¸å¿ƒä»·å€¼**: é€šè¿‡ç»Ÿä¸€ç°æœ‰çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹å¹¶æå‡ºä¸‰å¤§è®¾è®¡å‡†åˆ™ï¼Œæœ¬æ–‡å‘ç°åŠ¨æ€è®°å¿†ä¸é«˜æ•ˆé€¼è¿‘å¯å…±å­˜ï¼Œå¹¶æ®æ­¤æ„å»ºå‡ºæ»¡è¶³æ‰€æœ‰æ¡ä»¶çš„MetaLAï¼Œåœ¨ç†è®ºä¸Šå’Œå®éªŒä¸Šå‡å®ç°äº†å¯¹ç°æœ‰çº¿æ€§æ³¨æ„åŠ›çš„å…¨é¢è¶…è¶Šã€‚

**ğŸ“Š ä¸»ç±»åˆ«**: NeurIPS 2024 Oral

<details>
  <summary><b>ğŸ“– è¯¦ç»†åˆ†æ</b></summary>

#### ğŸ” é—®é¢˜ä¸æ´å¯Ÿ
- **æ ¹æœ¬é—®é¢˜**: ç°æœ‰çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹åœ¨é™ä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶ï¼Œæ— æ³•å…¼é¡¾åŠ¨æ€è®°å¿†èƒ½åŠ›ã€é™æ€é€¼è¿‘èƒ½åŠ›å’Œå‚æ•°æ•ˆç‡ï¼Œå¯¼è‡´æ€§èƒ½æ¬¡ä¼˜ã€‚
- **åˆ‡å…¥è§†è§’**: å°†å„ç±»çº¿æ€§å¤æ‚åº¦æ¨¡å‹ç»Ÿä¸€ä¸ºâ€˜çº¿æ€§æ³¨æ„åŠ›â€™å½¢å¼ï¼Œå¹¶æå‡ºä¸‰ä¸ªç†è®ºå¿…è¦æ¡ä»¶æ¥å®šä¹‰æœ€ä¼˜è®¾è®¡ï¼Œè€Œéä»…ä»ç»éªŒä¸Šæ”¹è¿›ç»“æ„ã€‚

#### âš™ï¸ æ–¹æ³•ä¸å‘ç°
- **å…³é”®æ–¹æ³•**: é€šè¿‡ç†è®ºåˆ†ææ¨å¯¼å‡ºæœ€ä¼˜çº¿æ€§æ³¨æ„åŠ›åº”æ»¡è¶³çš„ä¸‰ä¸ªæ¡ä»¶ï¼Œå¹¶æ®æ­¤æ„å»ºMetaLAï¼šå¼•å…¥å¯å­¦ä¹ çš„åŠ¨æ€æŠ•å½±æœºåˆ¶ä»¥åŒæ—¶å®ç°è¾“å…¥ä¾èµ–çš„æ˜ å°„ï¼ˆåŠ¨æ€è®°å¿†ï¼‰å’Œé«˜æ•ˆä½å‚é€¼è¿‘ï¼ˆæœ€å°å‚æ•°è¿‘ä¼¼ï¼‰ã€‚
- **æ–¹æ³•å…¬å¼**: MetaLA = åŠ¨æ€æŠ•å½±çŸ©é˜µ(è¾“å…¥) Ã— é”®/å€¼ç‰¹å¾ + æœ€å°å‚æ•°çº¦æŸ
- **æ ¸å¿ƒå‘ç°**: MetaLAæ˜¯é¦–ä¸ªåŒæ—¶æ»¡è¶³åŠ¨æ€è®°å¿†ã€é™æ€é€¼è¿‘ä¸æœ€å°å‚æ•°è¿‘ä¼¼çš„çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹ï¼Œåœ¨å¤šé¡¹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºLinFormerã€SSMç­‰ç°æœ‰æ–¹æ³•ã€‚

#### ğŸ’ ä»·å€¼è¯„ä¼°
- **æœºåˆ¶æ´å¯Ÿ**: æ­ç¤ºäº†çº¿æ€§æ³¨æ„åŠ›å¹¶éå•çº¯è¿½æ±‚ä½ç§©æˆ–å›ºå®šæŠ•å½±ï¼Œè€Œæ˜¯éœ€è¦åœ¨â€˜è¾“å…¥ä¾èµ–æ€§â€™ä¸â€˜å‚æ•°æ•ˆç‡â€™ä¹‹é—´å–å¾—å¹³è¡¡ï¼›æ‰“ç ´äº†â€˜çº¿æ€§=ç‰ºç‰²è¡¨è¾¾åŠ›â€™çš„è®¤çŸ¥ï¼Œè¯æ˜æ­£ç¡®è®¾è®¡ä¸‹å¯é€¼è¿‘softmaxæ³¨æ„åŠ›çš„æ ¸å¿ƒè¡Œä¸ºã€‚
- **è¡ŒåŠ¨å¯å‘**: è·¨è¶Šå¼ï¼ˆ5-10xæå‡ï¼‰ã€‚æä¾›æ˜ç¡®çš„è®¾è®¡å‡†åˆ™ï¼ˆä¸‰æ¡ä»¶ï¼‰å’Œå¯å¤ç”¨æ¨¡å—ï¼ˆåŠ¨æ€æŠ•å½±ï¼‰ï¼Œå¯ç›´æ¥æ›¿æ¢ç°æœ‰çº¿æ€§æ³¨æ„åŠ›ç»„ä»¶ï¼Œåœ¨é•¿åºåˆ—å»ºæ¨¡ä»»åŠ¡ä¸­å¿«é€ŸéªŒè¯æ€§èƒ½å¢ç›Šã€‚
- **å¯è¿ç§»æ€§**: 1. é«˜æ•ˆå›¾ç¥ç»ç½‘ç»œï¼ˆèŠ‚ç‚¹é—´ä¿¡æ¯ä¼ é€’çš„çº¿æ€§åŒ–é€¼è¿‘ï¼‰ï¼›2. æµå¼æ¨èç³»ç»Ÿï¼ˆç”¨æˆ·çŠ¶æ€çš„åŠ¨æ€çº¿æ€§è®°å¿†æ›´æ–°ï¼‰ï¼›3. ä¼ æ„Ÿå™¨ä¿¡å·å‹ç¼©å¤„ç†ï¼ˆä½å¤æ‚åº¦å®æ—¶ç‰¹å¾æå–ä¸­çš„é€¼è¿‘-æ•ˆç‡æƒè¡¡ï¼‰

#### ğŸ“„ åŸæ–‡æ‘˜è¦
Various linear complexity models, such as Linear Transformer (LinFormer), State Space Model (SSM), and Linear RNN (LinRNN), have been proposed to replace the conventional softmax attention in Transformer structures. However, the optimal design of these linear models is still an open question. In this work, we attempt to answer this question by finding the best linear approximation to softmax attention from a theoretical perspective. We start by unifying existing linear complexity models as the linear attention form and then identify three conditions for the optimal linear attention design: (1) Dynamic memory ability; (2) Static approximation ability; (3) Least parameter approximation. We find that none of the current linear models meet all three conditions, resulting in suboptimal performance. Instead, we propose Meta Linear Attention (MetaLA) as a solution that satisfies these conditions. Our experiments on Multi-Query Associative Recall (MQAR) task, language modeling, image classification, and Long-Range Arena (LRA) benchmark demonstrate that MetaLA is more effective than the existing linear models.

</details>


### [3] [A Taxonomy of Challenges to Curating Fair Datasets](https://neurips.cc/virtual/2024/oral/98019)
**é«˜ä»·å€¼** | *Dora Zhao, Morgan Scheuerman, Pooja Chitre, Jerone Andrews, Georgia Panagiotidou, Shawn Walker, Kathleen Pine, Alice Xiang*

**ğŸ’¡ å¤§ç™½è¯**: å°±åƒåšæ²™æ‹‰æ—¶å¤§å®¶åªå…³å¿ƒèœå¹²ä¸å¹²å‡€ï¼Œä½†æ²¡äººç®¡åˆ‡èœçš„äººç´¯ä¸ç´¯ã€å¬ä¸å¬å¾—æ‡‚é£Ÿè°±â€”â€”è¿™ç¯‡è®ºæ–‡å‘ç°ï¼Œè®©AIæ›´å…¬å¹³çš„å…³é”®ï¼Œæ˜¯å…ˆç…§é¡¾å¥½é‚£äº›é»˜é»˜æ•´ç†æ•°æ®çš„äººã€‚

**ğŸ¯ æ ¸å¿ƒä»·å€¼**: é€šè¿‡è®¿è°ˆ30ä½æ•°æ®æ•´ç†è€…å‘ç°ï¼Œæœºå™¨å­¦ä¹ å…¬å¹³æ€§çš„æ ¹æœ¬éšœç¢åœ¨äºç»„ç»‡ä¸å®è·µä¸­çš„éšæ€§æƒè¡¡è€ŒéæŠ€æœ¯ç¼ºé™·ï¼Œæå‡ºåº”ä»ç¤¾ä¼š-æŠ€æœ¯ç³»ç»Ÿè§’åº¦é‡æ„æ•°æ®æ•´ç†æµç¨‹ä»¥å®ç°ç³»ç»Ÿæ€§æ”¹è¿›ã€‚

**ğŸ“Š ä¸»ç±»åˆ«**: NeurIPS 2024 Oral

<details>
  <summary><b>ğŸ“– è¯¦ç»†åˆ†æ</b></summary>

#### ğŸ” é—®é¢˜ä¸æ´å¯Ÿ
- **æ ¹æœ¬é—®é¢˜**: æœºå™¨å­¦ä¹ æ•°æ®é›†å…¬å¹³æ€§æ”¹è¿›å—é˜»ï¼Œæ ¸å¿ƒçŸ›ç›¾åœ¨äºç¼ºä¹å¯¹å®é™…æ•°æ®æ•´ç†è¿‡ç¨‹ä¸­çœŸå®æŒ‘æˆ˜ä¸æƒè¡¡çš„ç³»ç»Ÿæ€§ç†è§£ã€‚
- **åˆ‡å…¥è§†è§’**: å…¬å¹³æ€§é—®é¢˜ä¸èƒ½ä»…ä»ç®—æ³•æˆ–ç»Ÿè®¡è§’åº¦è§£å†³ï¼Œå¿…é¡»æ·±å…¥åˆ°â€˜äººâ€™çš„æ“ä½œå±‚é¢â€”â€”å³æ•°æ®æ•´ç†è€…çš„å®è·µå›°å¢ƒä¸å†³ç­–é€»è¾‘ã€‚

#### âš™ï¸ æ–¹æ³•ä¸å‘ç°
- **å…³é”®æ–¹æ³•**: é€šè¿‡æ·±åº¦è®¿è°ˆ30åå®é™…ä»äº‹æœºå™¨å­¦ä¹ æ•°æ®æ•´ç†å·¥ä½œçš„ä»ä¸šè€…ï¼Œå½’çº³æç‚¼å‡ºè´¯ç©¿æ•´ä¸ªæ•°æ®æ•´ç†ç”Ÿå‘½å‘¨æœŸçš„æŒ‘æˆ˜ä¸æƒè¡¡ç±»å‹ï¼Œå¹¶æ„å»ºåˆ†ç±»ä½“ç³»ã€‚
- **æ–¹æ³•å…¬å¼**: å…¬å¹³æ€§æ´å¯Ÿ = è®¿è°ˆNä¸ªå®è·µè€… Ã— (æŒ‘æˆ˜è¯†åˆ« + æƒè¡¡æ˜ å°„) â†’ ç³»ç»Ÿæ€§åˆ†ç±»æ¡†æ¶
- **æ ¸å¿ƒå‘ç°**: æ•°æ®æ•´ç†ä¸­çš„å…¬å¹³æ€§éšœç¢ä¸»è¦æ¥è‡ªç»„ç»‡ã€æ²Ÿé€šã€èµ„æºåˆ†é…å’Œå®šä¹‰æ¨¡ç³Šç­‰éæŠ€æœ¯æ€§å› ç´ ï¼Œè€Œéå•çº¯çš„æ•°æ®åå·®æˆ–æ ‡æ³¨é”™è¯¯ã€‚

#### ğŸ’ ä»·å€¼è¯„ä¼°
- **æœºåˆ¶æ´å¯Ÿ**: æ­ç¤ºäº†ä¸€ä¸ªåç›´è§‰æœºåˆ¶ï¼šæå‡æ•°æ®å…¬å¹³æ€§çš„æœ€å¤§ç“¶é¢ˆä¸æ˜¯æŠ€æœ¯å·¥å…·ä¸è¶³ï¼Œè€Œæ˜¯â€˜éšæ€§åŠ³åŠ¨â€™æœªè¢«æ‰¿è®¤ã€è§’è‰²è¾¹ç•Œä¸æ¸…ã€è·¨å›¢é˜Ÿåä½œæ–­è£‚ã€‚è¿™æ”¹å˜äº†å°†â€˜å…¬å¹³â€™è§†ä¸ºçº¯æŠ€æœ¯ä¿®è¡¥çš„è®¤çŸ¥æ¨¡å‹ï¼Œè½¬å‘ç¤¾ä¼š-æŠ€æœ¯ç³»ç»Ÿè§†è§’ã€‚
- **è¡ŒåŠ¨å¯å‘**: è·¨è¶Šå¼ï¼ˆ5-10xæå‡ï¼‰ã€‚å»ºè®®ç«‹å³é‡æ„æ•°æ®å›¢é˜Ÿç»„ç»‡æ–¹å¼ï¼šâ‘  è®¾ç«‹â€˜æ•°æ®æ•´ç†åè°ƒå‘˜â€™è§’è‰²ï¼›â‘¡ åœ¨é¡¹ç›®åˆæœŸæ˜ç¡®å…¬å¹³æ€§è´£ä»»å½’å±ï¼›â‘¢ å°†æ•´ç†è¿‡ç¨‹æ—¥å¿—åŒ–ä»¥æ”¯æŒå®¡è®¡ã€‚è¿™äº›å¯åœ¨ä¸€ä¸ªè¿­ä»£å‘¨æœŸå†…éªŒè¯ã€‚
- **å¯è¿ç§»æ€§**: 1. è½¯ä»¶æµ‹è¯•æ•°æ®æ„å»ºï¼šæµ‹è¯•ç”¨ä¾‹é€‰æ‹©ä¸­çš„ä»£è¡¨æ€§æƒè¡¡ï¼›2. åŒ»ç–—æ•°æ®å…±äº«å¹³å°ï¼šéšç§ä¸å¯ç”¨æ€§çš„æ“ä½œå†²çªï¼›3. è‡ªåŠ¨é©¾é©¶åœºæ™¯åº“è®¾è®¡ï¼šè¾¹ç¼˜æ¡ˆä¾‹ä¼˜å…ˆçº§çš„è·¨éƒ¨é—¨åå•†æœºåˆ¶ã€‚

#### ğŸ“„ åŸæ–‡æ‘˜è¦
Despite extensive efforts to create fairer machine learning (ML) datasets, there remains a limited understanding of the practical aspects of dataset curation. Drawing from interviews with 30 ML dataset curators, we present a comprehensive taxonomy of the challenges and trade-offs encountered throughout the dataset curation lifecycle. Our findings underscore overarching issues within the broader fairness landscape that impact data curation. We conclude with recommendations aimed at fostering systemic changes to better facilitate fair dataset curation practices.

</details>


### [4] [ChaosBench: A Multi-Channel, Physics-Based Benchmark for Subseasonal-to-Seasonal Climate Prediction](https://neurips.cc/virtual/2024/oral/98017)
**é«˜ä»·å€¼** | *Juan Nathaniel, Yongquan Qu, Tung Nguyen, Sungduk Yu, Julius Busecke, Aditya Grover, Pierre Gentine*

**ğŸ’¡ å¤§ç™½è¯**: å°±åƒåªå­¦ä¼šçœ‹ä¸¤æ­¥æ£‹çš„äººå½“ä¸äº†è±¡æ£‹å¤§å¸ˆï¼Œç°åœ¨çš„å¤©æ°”AIåªä¼šç®—çŸ­æ—¶é—´ï¼Œä¸€ç®—å‡ ä¸ªæœˆå°±ä¹±æ¥ï¼›è¿™ä¸ªç ”ç©¶é€ äº†ä¸ªæ›´éš¾çš„è€ƒè¯•ï¼Œå‘ç°å®ƒä»¬å…¨ä¸åŠæ ¼ã€‚

**ğŸ¯ æ ¸å¿ƒä»·å€¼**: é’ˆå¯¹AIæ°”è±¡æ¨¡å‹éš¾ä»¥å»¶ä¼¸è‡³æ¬¡å­£èŠ‚å°ºåº¦çš„æ ¹æœ¬é—®é¢˜ï¼ŒChaosBenchæå‡ºé€šè¿‡å¤šåœˆå±‚é•¿åºåˆ—æ•°æ®ä¸ç‰©ç†ä¸€è‡´æ€§çº¦æŸé‡æ„è¯„ä¼°èŒƒå¼ï¼Œå‘ç°ç°æœ‰é¡¶å°–æ¨¡å‹å› å¿½è§†ç³»ç»Ÿè€¦åˆä¸å®ˆæ’å¾‹è€Œåœ¨é•¿æœŸé¢„æµ‹ä¸­å´©æºƒï¼Œæ­ç¤ºäº†æ•°æ®æ‹Ÿåˆä¸ç‰©ç†è‡ªæ´½é—´çš„æ·±å±‚çŸ›ç›¾ã€‚

**ğŸ“Š ä¸»ç±»åˆ«**: NeurIPS 2024 Oral

<details>
  <summary><b>ğŸ“– è¯¦ç»†åˆ†æ</b></summary>

#### ğŸ” é—®é¢˜ä¸æ´å¯Ÿ
- **æ ¹æœ¬é—®é¢˜**: ç°æœ‰æ•°æ®é©±åŠ¨å¤©æ°”æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›å±€é™äº15å¤©å†…ï¼Œç¼ºä¹å¯¹åœ°çƒç³»ç»Ÿå¤šåœˆå±‚è€¦åˆå’Œç‰©ç†ä¸€è‡´æ€§çš„å»ºæ¨¡ï¼Œå¯¼è‡´åœ¨æ¬¡å­£èŠ‚åˆ°å­£èŠ‚ï¼ˆS2Sï¼‰å°ºåº¦ä¸Šé¢„æµ‹å¤±æ•ˆã€‚
- **åˆ‡å…¥è§†è§’**: è¦å®ç°S2Så°ºåº¦å¯é é¢„æµ‹ï¼Œå¿…é¡»å°†æ•°æ®é©±åŠ¨æ¨¡å‹ç½®äºå®Œæ•´åœ°çƒç³»ç»Ÿï¼ˆå¤§æ°”ã€æµ·æ´‹ã€é™†åœ°ã€å†°ï¼‰çš„é•¿æœŸå†åˆ†ææ•°æ®ä¸­è®­ç»ƒï¼Œå¹¶é€šè¿‡ç‰©ç†çº¦æŸæŒ‡æ ‡è¯„ä¼°å…¶ä¸€è‡´æ€§ï¼Œè€Œéä»…ä¾èµ–çŸ­æœŸç¡®å®šæ€§è¯¯å·®ã€‚

#### âš™ï¸ æ–¹æ³•ä¸å‘ç°
- **å…³é”®æ–¹æ³•**: æ„å»ºåŒ…å«å¤šåœˆå±‚å˜é‡ã€45å¹´æ—¶åºè·¨åº¦çš„ChaosBenchåŸºå‡†ï¼Œå¼•å…¥ç‰©ç†å®ˆæ’å¾‹çº¦æŸçš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶ä¸å››å¤§æ°”è±¡æœºæ„çš„ç‰©ç†æ¨¡å‹åŠä¸»æµAIæ¨¡å‹å¯¹æ¯”æµ‹è¯•ã€‚
- **æ–¹æ³•å…¬å¼**: S2Så¯é¢„æµ‹æ€§ = (å¤šåœˆå±‚è€¦åˆè¾“å…¥ + é•¿å‘¨æœŸè®°å¿†) Ã— ç‰©ç†ä¸€è‡´æ€§æ­£åˆ™åŒ–
- **æ ¸å¿ƒå‘ç°**: ä¸“ä¸ºå¤©æ°”å°ºåº¦è®¾è®¡çš„AIæ¨¡å‹ï¼ˆå¦‚GraphCastã€PanguWeatherï¼‰åœ¨S2Sä»»åŠ¡ä¸Šæ€§èƒ½å´©æºƒï¼Œæ— æ³•ä¿æŒç‰©ç†åˆç†æ€§ï¼Œè¡¨æ˜å½“å‰AIæ°”è±¡æ¨¡å‹å­˜åœ¨è·¨æ—¶é—´å°ºåº¦æ³›åŒ–é¸¿æ²Ÿã€‚

#### ğŸ’ ä»·å€¼è¯„ä¼°
- **æœºåˆ¶æ´å¯Ÿ**: æ­ç¤ºäº†â€˜é«˜ç²¾åº¦çŸ­æœŸæ‹Ÿåˆâ€™ä¸â€˜é•¿æœŸç‰©ç†è‡ªæ´½â€™ä¹‹é—´çš„æ ¹æœ¬çŸ›ç›¾ï¼šæ¨¡å‹å¯ä»¥åœ¨å‡ å¤©å†…å®Œç¾æ‹Ÿåˆè§‚æµ‹ï¼Œä½†åœ¨æœˆ/å­£å°ºåº¦ä¸Šå› è¿åèƒ½é‡å®ˆæ’æˆ–åé¦ˆå¾ªç¯è€Œå‘æ•£ã€‚è¿™æ”¹å˜äº†AI for Scienceçš„è®¤çŸ¥æ¨¡å‹â€”â€”ä¸èƒ½ä»…ä»¥RMSEä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œå¿…é¡»å†…åµŒç‰©ç†å¾‹ã€‚
- **è¡ŒåŠ¨å¯å‘**: è·¨è¶Šå¼æ”¹è¿›ï¼ˆ>5xä»·å€¼æå‡ï¼‰ã€‚å»ºè®®ï¼šæ‰€æœ‰æ°”å€™AIæ¨¡å‹å¿…é¡»é€šè¿‡ChaosBenchç±»åŸºå‡†æµ‹è¯•ï¼›å¼€å‘æ–°æ¶æ„éœ€è”åˆä¼˜åŒ–é¢„æµ‹ç²¾åº¦ä¸ç‰©ç†å®ˆæ’è¯¯å·®ï¼›å¯ç«‹å³éªŒè¯è§„åˆ™ï¼šè‹¥æ¨¡å‹åœ¨ç¬¬30å¤©ä»æ»¡è¶³çƒ­åŠ›å­¦å¹³è¡¡è¯¯å·®<é˜ˆå€¼ï¼Œåˆ™å…·å¤‡S2Sæ½œåŠ›ã€‚
- **å¯è¿ç§»æ€§**: 1. é‡‘èé£é™©é¢„æµ‹ï¼ˆå¸‚åœº-æ”¿ç­–-èˆ†æƒ…å¤šç³»ç»Ÿè€¦åˆ+é•¿å°¾äº‹ä»¶å»ºæ¨¡ï¼‰ï¼›2. æµè¡Œç—…è·¨å­£èŠ‚ä¼ æ’­ï¼ˆæ°”å€™-äººå£æµåŠ¨-å…ç–«è¡°å‡è”åˆåŠ¨åŠ›å­¦ï¼‰ï¼›3. å¤šæ™ºèƒ½ä½“ç¤¾ä¼šæ¨¡æ‹Ÿï¼ˆä¸ªä½“è¡Œä¸º-åˆ¶åº¦åé¦ˆ-æ–‡åŒ–æ¼”åŒ–çš„éçº¿æ€§ç´¯ç§¯æ•ˆåº”ï¼‰

#### ğŸ“„ åŸæ–‡æ‘˜è¦
Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster preparedness and robust decision making amidst climate change. Yet, forecasting beyond the weather timescale is challenging because it deals with problems other than initial condition, including boundary interaction, butterfly effect, and our inherent lack of physical understanding. At present, existing benchmarks tend to have shorter forecasting range of up-to 15 days, do not include a wide range of operational baselines, and lack physics-based constraints for explainability. Thus, we propose ChaosBench, a challenging benchmark to extend the predictability range of data-driven weather emulators to S2S timescale. First, ChaosBench is comprised of variables beyond the typical surface-atmospheric ERA5 to also include ocean, ice, and land reanalysis products that span over 45 years to allow for full Earth system emulation that respects boundary conditions. We also propose physics-based, in addition to deterministic and probabilistic metrics, to ensure a physically-consistent ensemble that accounts for butterfly effect. Furthermore, we evaluate on a diverse set of physics-based forecasts from four national weather agencies as baselines to our data-driven counterpart such as ViT/ClimaX, PanguWeather, GraphCast, and FourCastNetV2. Overall, we find methods originally developed for weather-scale applications fail on S2S task: their performance simply collapse to â€¦

</details>


### [5] [Graph Diffusion Transformers for Multi-Conditional Molecular Generation](https://neurips.cc/virtual/2024/oral/97964)
**é«˜ä»·å€¼** | *Gang Liu, Jiaxin Xu, Tengfei Luo, Meng Jiang*

**ğŸ’¡ å¤§ç™½è¯**: å°±åƒæ•™AIåšâ€˜æŒ‰è¦æ±‚æ­ç§¯æœ¨â€™ï¼Œä»¥å‰æ˜¯éšä¾¿ä¹±æ‰”é›¶ä»¶å†æ‹¼ï¼Œç°åœ¨æ˜¯çœ‹å›¾çº¸ä¸€æ­¥æ­¥ç²¾å‡†è°ƒæ•´ï¼Œè¿˜èƒ½å¬æ‡‚â€˜è¦é€æ°”åˆç»“å®â€™è¿™ç§å¤šä¸ªè¦æ±‚ä¸€èµ·æã€‚

**ğŸ¯ æ ¸å¿ƒä»·å€¼**: é’ˆå¯¹å¤šå±æ€§æ¡ä»¶ä¸‹çš„åˆ†å­ç”Ÿæˆéš¾é¢˜ï¼Œæœ¬æ–‡é€šè¿‡å¼•å…¥å›¾ä¾èµ–å™ªå£°æ¨¡å‹ä¸ç»Ÿä¸€æ¡ä»¶ç¼–ç çš„Transformeræ¶æ„ï¼Œå®ç°äº†å¯¹åˆ†å­å›¾ç»“æ„ä¸æ€§èƒ½çš„ååŒç²¾ç¡®æ§åˆ¶ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆåˆ†å­åœ¨çœŸå®è®¾è®¡ä»»åŠ¡ä¸­çš„å¯ç”¨æ€§ã€‚

**ğŸ“Š ä¸»ç±»åˆ«**: NeurIPS 2024 Oral

<details>
  <summary><b>ğŸ“– è¯¦ç»†åˆ†æ</b></summary>

#### ğŸ” é—®é¢˜ä¸æ´å¯Ÿ
- **æ ¹æœ¬é—®é¢˜**: ç°æœ‰å›¾æ‰©æ•£æ¨¡å‹åœ¨å¤šæ¡ä»¶åˆ†å­ç”Ÿæˆä¸­æ— æ³•æœ‰æ•ˆæ•´åˆæ•°å€¼å‹ä¸ç±»åˆ«å‹å±æ€§ï¼ˆå¦‚åˆæˆéš¾åº¦ã€æ°”ä½“æ¸—é€æ€§ï¼‰ï¼Œä¸”å™ªå£°å»ºæ¨¡æ–¹å¼å‰²è£‚åŸå­ä¸é”®çš„å…³è”ï¼Œå¯¼è‡´æ¡ä»¶æ§åˆ¶å¼±ã€ç”Ÿæˆè´¨é‡ä½ã€‚
- **åˆ‡å…¥è§†è§’**: å°†åˆ†å­å›¾çš„ç»“æ„ä¾èµ–æ€§æ˜¾å¼å»ºæ¨¡åˆ°å™ªå£°ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå¹¶é€šè¿‡ç»Ÿä¸€çš„æ¡ä»¶ç¼–ç å™¨èåˆå¤šæ¨¡æ€å±æ€§ä¿¡æ¯ï¼Œå¯å®ç°æ›´ç²¾å‡†çš„æ¡ä»¶å¼•å¯¼ç”Ÿæˆã€‚

#### âš™ï¸ æ–¹æ³•ä¸å‘ç°
- **å…³é”®æ–¹æ³•**: æå‡ºGraph-dependentå™ªå£°æ¨¡å‹ï¼Œåœ¨å‰å‘æ‰©æ•£ä¸­æ ¹æ®å›¾ç»“æ„åŠ¨æ€ç”Ÿæˆå™ªå£°ï¼›è®¾è®¡åŸºäºTransformerçš„å›¾å»å™ªå™¨ï¼Œç»“åˆæ¡ä»¶ç¼–ç å™¨å¯¹å¤šå±æ€§è”åˆç¼–ç è¿›è¡Œæ¡ä»¶æ§åˆ¶ã€‚
- **æ–¹æ³•å…¬å¼**: æ¡ä»¶ç”Ÿæˆ = Transformerå»å™ªå™¨(å›¾ç»“æ„ + ç»“æ„ä¾èµ–å™ªå£°, æ¡ä»¶åµŒå…¥[å±æ€§1, å±æ€§2...])
- **æ ¸å¿ƒå‘ç°**: Graph DiTåœ¨èšåˆç‰©ä¸å°åˆ†å­çš„å¤šæ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨å±æ€§æ§åˆ¶ç²¾åº¦å’Œåˆ†å¸ƒä¿çœŸåº¦ä¸Šè¡¨ç°çªå‡ºï¼Œå¹¶ç»ä¸“å®¶åé¦ˆéªŒè¯å…¶åœ¨çœŸå®æ°”ä½“åˆ†ç¦»ææ–™è®¾è®¡ä¸­çš„å¯ç”¨æ€§ã€‚

#### ğŸ’ ä»·å€¼è¯„ä¼°
- **æœºåˆ¶æ´å¯Ÿ**: æ­ç¤ºäº†â€˜ç»“æ„æ„ŸçŸ¥å™ªå£°â€™æ¯”ç‹¬ç«‹åŠ å™ªæ›´èƒ½ä¿ç•™åˆ†å­æ‹“æ‰‘çº¦æŸçš„åç›´è§‰æœºåˆ¶â€”â€”ä¼ ç»Ÿæ‰©æ•£å‡è®¾åŸå­/é”®ç‹¬ç«‹æ‰°åŠ¨ç ´åäº†åŒ–å­¦åˆç†æ€§ï¼Œè€Œå›¾ä¾èµ–å™ªå£°ç»´æŒäº†å…³é”®å­ç»“æ„ç¨³å®šæ€§ï¼Œæ”¹å˜äº†åˆ†å­æ‰©æ•£æ¨¡å‹çš„è®¾è®¡èŒƒå¼ã€‚
- **è¡ŒåŠ¨å¯å‘**: è·¨è¶Šå¼æ”¹è¿›ï¼ˆ5-10xæå‡ï¼‰ã€‚æä¾›å¯æ“ä½œè§„åˆ™ï¼š1ï¼‰åœ¨åˆ†å­æ‰©æ•£ä¸­ä¼˜å…ˆä½¿ç”¨å›¾æ„ŸçŸ¥å™ªå£°ï¼›2ï¼‰å¤šæ¡ä»¶è¾“å…¥åº”ç»Ÿä¸€ç¼–ç ä¸ºè”åˆæ¡ä»¶åµŒå…¥ï¼›3ï¼‰ç”¨Transformeræ›¿ä»£GNNä½œä¸ºå»å™ªä¸»å¹²ã€‚å¯åœ¨è¯ç‰©/ææ–™ç”Ÿæˆæµç¨‹ä¸­ç›´æ¥æ›¿æ¢ç°æœ‰æ‰©æ•£æ¨¡å—ã€‚
- **å¯è¿ç§»æ€§**: 1ï¼‰ç”µæ± ææ–™è®¾è®¡ï¼ˆæ¡ä»¶ï¼šç¦»å­ç”µå¯¼ç‡+å¾ªç¯å¯¿å‘½ï¼‰ï¼›2ï¼‰å‚¬åŒ–å‰‚é€†å‘è®¾è®¡ï¼ˆæ¡ä»¶ï¼šæ´»æ€§ä½ç‚¹å¯†åº¦+ç¨³å®šæ€§ï¼‰ï¼›3ï¼‰åŸå¸‚äº¤é€šç½‘ç»œç”Ÿæˆï¼ˆæ¡ä»¶ï¼šæµé‡å®¹é‡+å»ºè®¾æˆæœ¬ï¼‰â€”â€”ä¸‰è€…å‡æ¶‰åŠå›¾ç»“æ„å¯¹è±¡ä¸å¤šç»´æ€§èƒ½æŒ‡æ ‡çš„è”åˆä¼˜åŒ–ã€‚

#### ğŸ“„ åŸæ–‡æ‘˜è¦
Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecule generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We present the Graph Diffusion Transformer (Graph DiT) for multi-conditional molecular generation. Graph DiT has a condition encoder to learn the representation of numerical and categorical properties and utilizes a Transformer-based graph denoiser to achieve molecular graph denoising under conditions. Unlike previous graph diffusion models that add noise separately on the atoms and bonds in the forward diffusion process, we propose a graph-dependent noise model for training Graph DiT, designed to accurately estimate graph-related noise in molecules. We extensively validate the Graph DiT for multi-conditional polymer and small molecule generation. Results demonstrate our superiority across metrics from distribution learning to condition control for molecular properties. A polymer inverse design task for gas separation with feedback from domain experts further demonstrates its practical utility. The code is available at https://github.com/liugangcode/Graph-DiT.

</details>


### [6] [Bayesian-guided Label Mapping for Visual Reprogramming](https://neurips.cc/virtual/2024/oral/98002)
**é«˜ä»·å€¼** | *Chengyi Cai, Zesheng Ye, Lei Feng, Jianzhong Qi, Feng Liu*

**ğŸ’¡ å¤§ç™½è¯**: å°±åƒç”¨çŒœè°œæ¸¸æˆçš„ç­”æ¡ˆåˆ†å¸ƒæ¥åæ¨é¢˜ç›®ç±»å‹ï¼Œè€Œä¸æ˜¯æ­»è®°ç¡¬èƒŒç­”æ¡ˆå¯¹â€”â€”è¿™ä¸ªæ–¹æ³•æ•™ä¼šAIç”¨â€˜å¯èƒ½æ€§â€™æ€ç»´æŠŠæ—§çŸ¥è¯†çµæ´»ç”¨åœ¨æ–°é—®é¢˜ä¸Šã€‚

**ğŸ¯ æ ¸å¿ƒä»·å€¼**: é’ˆå¯¹è§†è§‰é‡ç¼–ç¨‹ä¸­æ ‡ç­¾ç©ºé—´é”™é…çš„æ ¹æœ¬é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä»¥è´å¶æ–¯åéªŒæ¦‚ç‡åŠ¨æ€å»ºæ¨¡é¢„è®­ç»ƒä¸ä¸‹æ¸¸æ ‡ç­¾é—´çš„å¤æ‚å…³ç³»ï¼Œé€šè¿‡æ— éœ€æ¢¯åº¦çš„æ¦‚ç‡æ˜ å°„çŸ©é˜µå®ç°äº†æ›´ä¼˜çš„ä»»åŠ¡é€‚é…æ€§èƒ½ã€‚

**ğŸ“Š ä¸»ç±»åˆ«**: NeurIPS 2024 Oral

<details>
  <summary><b>ğŸ“– è¯¦ç»†åˆ†æ</b></summary>

#### ğŸ” é—®é¢˜ä¸æ´å¯Ÿ
- **æ ¹æœ¬é—®é¢˜**: ç°æœ‰è§†è§‰é‡ç¼–ç¨‹ä¸­çš„æ ‡ç­¾æ˜ å°„æ–¹æ³•é‡‡ç”¨ä¸€å¯¹ä¸€ç¡¬åŒ¹é…ï¼Œæ— æ³•æ•æ‰é¢„è®­ç»ƒæ ‡ç­¾ä¸ä¸‹æ¸¸ä»»åŠ¡æ ‡ç­¾ä¹‹é—´å¤æ‚çš„å¤šå¯¹å¤šå…³ç³»ï¼Œå¯¼è‡´è¯­ä¹‰é¸¿æ²Ÿä¸‹çš„æ€§èƒ½ç“¶é¢ˆã€‚
- **åˆ‡å…¥è§†è§’**: æ ‡ç­¾é—´çš„æ˜ å°„åº”æ˜¯æ¦‚ç‡æ€§çš„ã€å¯è¿­ä»£æ›´æ–°çš„è”åˆåˆ†å¸ƒå»ºæ¨¡é—®é¢˜ï¼Œè€Œéé™æ€çš„ç¡®å®šæ€§å¯¹åº”ã€‚

#### âš™ï¸ æ–¹æ³•ä¸å‘ç°
- **å…³é”®æ–¹æ³•**: æ„å»ºä¸€ä¸ªç”±è´å¶æ–¯æ¡ä»¶æ¦‚ç‡æŒ‡å¯¼çš„åŠ¨æ€æ¦‚ç‡æ ‡ç­¾æ˜ å°„çŸ©é˜µï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ ¹æ®æ¨¡å‹åœ¨ä¸‹æ¸¸æ ·æœ¬ä¸Šçš„é¢„æµ‹åˆ†å¸ƒä¸æ–­ä¼˜åŒ–è¯¥çŸ©é˜µã€‚
- **æ–¹æ³•å…¬å¼**: æ¦‚ç‡æ˜ å°„çŸ©é˜µ = è´å¶æ–¯åéªŒP(ä¸‹æ¸¸æ ‡ç­¾|é¢„è®­ç»ƒæ ‡ç­¾, ä¸‹æ¸¸æ•°æ®é¢„æµ‹åˆ†å¸ƒ)
- **æ ¸å¿ƒå‘ç°**: é€šè¿‡å»ºæ¨¡é¢„è®­ç»ƒæ ‡ç­¾ä¸ä¸‹æ¸¸æ ‡ç­¾ä¹‹é—´çš„è½¯ã€æ¦‚ç‡åŒ–å¯¹åº”å…³ç³»ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰é‡ç¼–ç¨‹åœ¨è·¨æ ‡ç­¾ç©ºé—´ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§ï¼Œå¹¶æ­ç¤ºäº†æ ‡ç­¾å¯¹é½çš„æœ¬è´¨æ˜¯åˆ†å¸ƒå¯¹é½ã€‚

#### ğŸ’ ä»·å€¼è¯„ä¼°
- **æœºåˆ¶æ´å¯Ÿ**: æ­ç¤ºäº†â€˜æ ‡ç­¾æ˜ å°„â€™æœ¬è´¨ä¸Šæ˜¯å¯¹é½ä¸¤ä¸ªæ ‡ç­¾ç©ºé—´çš„è”åˆåˆ†å¸ƒï¼Œè€Œéç®€å•åŒ¹é…åç§°ã€‚è¿™ç§åç›´è§‰çš„è®¤çŸ¥è½¬å˜è¡¨æ˜ï¼šå³ä½¿æ²¡æœ‰å¾®è°ƒï¼Œæ¨¡å‹å†…éƒ¨å·²è•´å«å¯ç”¨äºæ¨æ–­ç›®æ ‡ä»»åŠ¡ç»“æ„çš„ç»Ÿè®¡ä¿¡å·ï¼Œå…³é”®åœ¨äºå¦‚ä½•ç”¨æ­£ç¡®æœºåˆ¶æå–å®ƒã€‚
- **è¡ŒåŠ¨å¯å‘**: è·¨è¶Šå¼ï¼ˆ5-10xæå‡ï¼‰ã€‚æä¾›å¯ç«‹å³åº”ç”¨çš„è´å¶æ–¯æ ¡å‡†æ¨¡å—ï¼š1ï¼‰å¯¹ä»»ä½•é¢„è®­ç»ƒæ¨¡å‹+ä¸‹æ¸¸æ•°æ®é›†ï¼Œå…ˆè¿è¡Œå‰å‘æ¨ç†æ”¶é›†é¢„æµ‹åˆ†å¸ƒï¼›2ï¼‰åŸºäºä¸‹æ¸¸ç±»åˆ«å…ˆéªŒå’Œé¢„æµ‹ç»“æœè®¡ç®—åéªŒæ˜ å°„çŸ©é˜µï¼›3ï¼‰ç”¨äºæœ€ç»ˆå†³ç­–ã€‚æ— éœ€æ¢¯åº¦ã€ä¸ä¿®æ”¹åŸæ¨¡å‹ã€‚
- **å¯è¿ç§»æ€§**: 1ï¼‰è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é›¶æ ·æœ¬åˆ†ç±»å™¨é€‚é…ï¼ˆå¦‚å°†BERTçš„æ©ç é¢„æµ‹ç©ºé—´æ˜ å°„åˆ°æƒ…æ„Ÿåˆ†ç±»ï¼‰ï¼›2ï¼‰ä¼ æ„Ÿå™¨å¼‚æ„ç³»ç»Ÿä¸­çš„ä¿¡å·è¯­ä¹‰å¯¹é½ï¼ˆå¦‚å°†çº¢å¤–å›¾åƒæ ‡ç­¾æ˜ å°„åˆ°å¯è§å…‰ä»»åŠ¡ï¼‰ï¼›3ï¼‰è·¨æ¨¡æ€æ£€ç´¢ä¸­æ¨¡æ€é—´æ¦‚å¿µçš„æ¦‚ç‡å…³è”å»ºæ¨¡ï¼ˆå¦‚æ–‡æœ¬çŸ­è¯­â†’å›¾åƒåŒºåŸŸï¼‰ã€‚

#### ğŸ“„ åŸæ–‡æ‘˜è¦
*Visual reprogramming* (VR) leverages the intrinsic capabilities of pretrained vision models by adapting their input or output interfaces to solve downstream tasks whose labels (i.e., downstream labels) might be totally different from the labels associated with the pretrained models (i.e., pretrained labels). When adapting the output interface, label mapping methods transform the pretrained labels to downstream labels by establishing a gradient-free one-to-one correspondence between the two sets of labels.However, in this paper, we reveal that one-to-one mappings may overlook the complex relationship between pretrained and downstream labels. Motivated by this observation, we propose a ***B**ayesian-guided **L**abel **M**apping* (BLM) method. BLM constructs an iteratively-updated probabilistic label mapping matrix, with each element quantifying a pairwise relationship between pretrained and downstream labels.The assignment of values to the constructed matrix is guided by Bayesian conditional probability, considering the joint distribution of the downstream labels and the labels predicted by the pretrained model on downstream samples. Experiments conducted on both pretrained vision models (e.g., ResNeXt) and vision-language models (e.g., CLIP) demonstrate the superior performance of BLM over existing label mapping methods. The success of BLM also offers a probabilistic lens through which to understand and analyze the effectiveness of VR.Our code is available at https://github.com/tmlr-group/BayesianLM.

</details>
